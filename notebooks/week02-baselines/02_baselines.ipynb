{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "40f7aaa5",
   "metadata": {},
   "source": [
    "# Week 2 — Simple ML Baselines + Feature Engineering\n",
    "\n",
    "**Learning Goals:**\n",
    "- Build reproducible baselines with classical ML (scikit-learn)\n",
    "- Feature engineering for time series → tabular format\n",
    "- Establish metric baselines: MAE, RMSE, NASA Score\n",
    "\n",
    "**Models:** Linear Regression, Ridge, Random Forest, Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c08bb40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, '../../src')\n",
    "from data.data_loader import load_train, load_test\n",
    "from data.preprocess import INFORMATIVE_SENSORS_FD001, train_val_split, fit_scaler, apply_scaler\n",
    "from data.features import extract_features, extract_windowed_features\n",
    "from train import compute_metrics\n",
    "\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "print('Imports OK')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "047c2708",
   "metadata": {},
   "source": [
    "## 1. Load & Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74e0bdca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "df_train_raw = load_train(fd_number=1, rul_cap=125)\n",
    "df_test_raw, rul_true = load_test(fd_number=1)\n",
    "\n",
    "# Train/val split by engine\n",
    "df_train, df_val = train_val_split(df_train_raw, val_fraction=0.2, random_state=42)\n",
    "print(f'Train engines: {df_train[\"unit_id\"].nunique()}, Val engines: {df_val[\"unit_id\"].nunique()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47e2e6fe",
   "metadata": {},
   "source": [
    "## 2. Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e33b3d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features (one row per engine, using last 30 cycles)\n",
    "WINDOW = 30\n",
    "sensors = INFORMATIVE_SENSORS_FD001\n",
    "\n",
    "feat_train = extract_features(df_train, sensors, window=WINDOW)\n",
    "feat_val = extract_features(df_val, sensors, window=WINDOW)\n",
    "feat_test = extract_features(df_test_raw, sensors, window=WINDOW)\n",
    "\n",
    "print(f'Train features shape: {feat_train.shape}')\n",
    "print(f'Val features shape:   {feat_val.shape}')\n",
    "print(f'Test features shape:  {feat_test.shape}')\n",
    "print(f'\\nFeature columns ({len(feat_train.columns)}): {list(feat_train.columns[:10])}...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b6d4144",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare X, y arrays\n",
    "feature_cols = [c for c in feat_train.columns if c not in ['unit_id', 'RUL']]\n",
    "\n",
    "X_train = feat_train[feature_cols].values\n",
    "y_train = feat_train['RUL'].values\n",
    "\n",
    "X_val = feat_val[feature_cols].values\n",
    "y_val = feat_val['RUL'].values\n",
    "\n",
    "X_test = feat_test[feature_cols].values\n",
    "y_test = rul_true\n",
    "\n",
    "print(f'X_train: {X_train.shape}, y_train: {y_train.shape}')\n",
    "print(f'X_val:   {X_val.shape}, y_val: {y_val.shape}')\n",
    "print(f'X_test:  {X_test.shape}, y_test: {y_test.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aee6633e",
   "metadata": {},
   "source": [
    "## 3. Train Baseline Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3838d199",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define models\n",
    "models = {\n",
    "    'Linear Regression': LinearRegression(),\n",
    "    'Ridge (alpha=10)': Ridge(alpha=10),\n",
    "    'Random Forest': RandomForestRegressor(n_estimators=100, max_depth=10, random_state=42, n_jobs=-1),\n",
    "    'Gradient Boosting': GradientBoostingRegressor(n_estimators=200, max_depth=5, learning_rate=0.1, random_state=42),\n",
    "}\n",
    "\n",
    "# Train and evaluate each model\n",
    "results = []\n",
    "predictions = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f'\\n--- {name} ---')\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Validation predictions\n",
    "    y_val_pred = model.predict(X_val)\n",
    "    val_metrics = compute_metrics(y_val, y_val_pred)\n",
    "    \n",
    "    # Test predictions \n",
    "    y_test_pred = model.predict(X_test)\n",
    "    test_metrics = compute_metrics(y_test, y_test_pred)\n",
    "    \n",
    "    print(f'  Val  — MAE: {val_metrics[\"MAE\"]:.2f}, RMSE: {val_metrics[\"RMSE\"]:.2f}')\n",
    "    print(f'  Test — MAE: {test_metrics[\"MAE\"]:.2f}, RMSE: {test_metrics[\"RMSE\"]:.2f}, NASA: {test_metrics[\"NASA_Score\"]:.0f}')\n",
    "    \n",
    "    results.append({\n",
    "        'Model': name,\n",
    "        'Val MAE': val_metrics['MAE'],\n",
    "        'Val RMSE': val_metrics['RMSE'],\n",
    "        'Test MAE': test_metrics['MAE'],\n",
    "        'Test RMSE': test_metrics['RMSE'],\n",
    "        'Test NASA Score': test_metrics['NASA_Score'],\n",
    "    })\n",
    "    predictions[name] = y_test_pred\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "542cc7d1",
   "metadata": {},
   "source": [
    "## 4. Visualize Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6e163bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction vs True RUL for each model\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "for idx, (name, y_pred) in enumerate(predictions.items()):\n",
    "    ax = axes[idx // 2, idx % 2]\n",
    "    ax.scatter(y_test, y_pred, alpha=0.6, s=30, edgecolors='none')\n",
    "    lims = [0, max(y_test.max(), y_pred.max()) + 10]\n",
    "    ax.plot(lims, lims, 'r--', linewidth=1)\n",
    "    ax.set_xlabel('True RUL')\n",
    "    ax.set_ylabel('Predicted RUL')\n",
    "    metrics = compute_metrics(y_test, y_pred)\n",
    "    ax.set_title(f'{name}\\nMAE={metrics[\"MAE\"]:.1f}, RMSE={metrics[\"RMSE\"]:.1f}')\n",
    "\n",
    "plt.suptitle('Baseline Models — Predicted vs True RUL (FD001 Test)', fontsize=14, y=1.01)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c5631c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Error analysis: which engines are hardest to predict?\n",
    "best_model_name = results_df.loc[results_df['Test MAE'].idxmin(), 'Model']\n",
    "best_preds = predictions[best_model_name]\n",
    "errors = best_preds - y_test\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "axes[0].bar(range(len(errors)), errors, color=['red' if e > 0 else 'blue' for e in errors], alpha=0.7)\n",
    "axes[0].set_xlabel('Engine ID')\n",
    "axes[0].set_ylabel('Prediction Error (pred - true)')\n",
    "axes[0].set_title(f'{best_model_name} — Error per Engine')\n",
    "axes[0].axhline(0, color='black', linewidth=0.5)\n",
    "\n",
    "axes[1].hist(errors, bins=20, edgecolor='black', alpha=0.7, color='steelblue')\n",
    "axes[1].axvline(0, color='red', linestyle='--')\n",
    "axes[1].set_xlabel('Prediction Error')\n",
    "axes[1].set_ylabel('Count')\n",
    "axes[1].set_title(f'{best_model_name} — Error Distribution')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52e57b6c",
   "metadata": {},
   "source": [
    "## 5. Feature Importance (Best Model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "110a9c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance from Random Forest\n",
    "rf_model = models['Random Forest']\n",
    "importance = rf_model.feature_importances_\n",
    "feat_imp = pd.Series(importance, index=feature_cols).sort_values(ascending=False)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "feat_imp.head(20).plot(kind='barh', ax=ax, color='steelblue', edgecolor='black')\n",
    "ax.set_xlabel('Feature Importance')\n",
    "ax.set_title('Random Forest — Top 20 Feature Importances')\n",
    "ax.invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d240f38",
   "metadata": {},
   "source": [
    "## 6. Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb6add46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results table\n",
    "results_df.to_csv('../../reports/baseline_results.csv', index=False)\n",
    "print('Baseline results saved to reports/baseline_results.csv')\n",
    "print()\n",
    "print(results_df.to_markdown(index=False))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
